Hi — I’m a computer science student and this repo is my compact, hands‑on take on the SEAL (Self‑Adapting Language) idea. I wrote the code and experiments here to learn how a model can generate its own training examples and then fine‑tune itself with those examples.

This README explains what I tried, why I simplified the method, and how to run the code on a modest machine (CPU or a low‑VRAM GPU).

TL;DR: I implement a small-scale workflow where a teacher model (or a prompt) writes synthetic training examples and short directives. I fine‑tune a small student model using LoRA/adapters and then evaluate whether the student improves at answering questions without seeing the original passage.

---

What this repo contains (short)
- `Mini_SEAL.ipynb`: Notebook with the full demo (data generation, self-edits, LoRA fine‑tuning, evaluation).
- `self_edits.json`: Synthetic examples generated by the teacher (used for fine‑tuning).
- `questionAndanswers.json`: My small evaluation set (questions and ground-truth answers).
- `requirements.txt`: Minimal dependencies for CPU-friendly runs (transformers, torch, peft, accelerate).

Design choices I made (because I have limited hardware)
- I use small open-source models (e.g., `distilgpt2`, `pythia-160m`) so experiments run on CPU or small GPUs.
- I use LoRA/adapters to keep fine‑tuning cheap: only a small fraction of parameters change.
- I run just one or two rounds of: generate self‑edits → fine‑tune → evaluate. This shows the method without requiring massive compute.

Simplified workflow (how I ran experiments)
1) Pick a short passage and human-annotated questions+answers (my evaluation set τ).
2) Generate multiple self‑edit variants (synthetic Q/A or concise statements) with a stronger teacher model or a careful prompt.
3) Aggregate the synthetic examples into `self_edits.json` and extract a simple training directive (epochs, lr, batch size). If the directive is too large for my hardware, I use a small default.
4) Fine‑tune the student model with LoRA using the synthetic examples.
5) Evaluate the student on the original questions (without providing the passage). Compare metrics (Exact Match, token-level F1) to the baseline.

How to reproduce (quick)
1) Create and activate a Python venv and install requirements (PowerShell example):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install -r requirements.txt
```

2) Run the notebook `Mini_SEAL.ipynb` in VSCode or Jupyter. The key sections are:
- Model loading and baseline generation
- Self‑edit generation (uses `self_edits.json` if you already have it)
- LoRA fine‑tuning (small epochs, small batch sizes)
- Evaluation and metrics comparison

Notes for very small hardware
- Set `num_train_epochs=1-2`, `per_device_train_batch_size=1-2`, `max_length` small (<=128).
- If you have no GPU, set `device_map='cpu'` and avoid fp16. LoRA still helps because fewer parameters are updated.
- If a model download is too big, use `distilgpt2` or `pythia-70m-deduped` where possible.

Security and repo hygiene
- Don’t commit API keys. I keep keys in environment variables and have a `.gitignore` to avoid committing virtual environments and model checkpoints.

What I learned / caveats
- Self-edits can help, but improvements depend a lot on (1) quality of synthetic examples and (2) how well the directive matches the available compute.
- Small models can learn to answer questions better after a few targeted LoRA updates, but they also risk forgetting unrelated knowledge.

If you want, I can also:
- Add a short `run.sh` / PowerShell script with the exact commands I used for training and evaluation.
- Add a tiny `demo/` folder that contains a minimal script you can run on CPU that does generation → LoRA → eval with a couple of examples.

Tell me which one you prefer and what hardware you have (CPU only, or GPU with X GB VRAM). I’ll prepare the next steps accordingly.

---

---

## What SEAL *really does* (in simple terms)

Here are the core ideas of the full SEAL method, phrased plainly:

1. You have a base language model (LM) trained on lots of general data. ([arXiv][1])
2. When the model sees *new information* (for example: a new text passage it has not seen before), it doesn’t just passively store it or rely on in-context only; instead the model **writes its own training examples** (synthetic data) and also **writes instructions** (like: how many epochs to train, what learning rate) for how to fine-tune itself. These are called “self-edits”. ([jyopari.github.io][2])
3. The model then uses those self-edits to fine‐tune itself (i.e., update its weights) so the information becomes “internalised” rather than just in the prompt. ([arXiv][3])
4. After that fine-tuning, the model is tested on the downstream task (for example: answer questions about the new info without seeing the original passage). The performance gives a reward signal. If the performance improved, the “self-edit” strategy is deemed good. ([jyopari.github.io][2])
5. Using that reward, there is a meta‐learning / reinforcement learning (RL) outer loop: the model learns *how* to write better self‐edits and instructions over time (generate data + update strategy that yield better results). ([arXiv][1])
6. Repeat: new data → self‐edit → fine‐tune → evaluate → update edit-policy → new data, etc. This yields a model that *can adapt itself*, not just respond via prompt but actually change its weights. ([syncedreview.com][4])

**Why this is cool**: It mimics how a human student might learn: sees new info → writes notes or practice questions → studies them → tests themselves → improves how they study next time. The model is doing analogous steps. ([ResearchGate][5])

**Why it’s hard**:

* You need a credible downstream task with ground truth for evaluation (so you can compute reward).
* You need enough resources to fine-tune the model, generate many self-edits, evaluate repeatedly (which is compute‐heavy).
* There is risk of “forgetting” old info (catastrophic forgetting) when new updates overwrite old knowledge. ([arXiv][3])
* The full RL outer loop is complex; you may simplify.

---

## How you can *simplify* SEAL for your hardware & skills

Since you’re a student with a simple computer (less powerful GPU or maybe CPU), you should scale this down. Here’s how to adapt SEAL to be feasible:

* Use a **small language model** rather than huge LLMs (reduces compute, memory).
* Use **only one or two rounds** of “self-edit → fine‐tune → evaluate”, rather than many outer RL loops.
* Skip or heavily simplify the RL component: instead of a full policy gradient or PPO, pick the best self‐edit out of a small set, or manually analyse which edits improve performance; you don’t need to implement complex RL.
* Use a **small downstream task**: e.g., a short passage plus a few questions, or a classification task, rather than large benchmarks.
* Use **adapter/LoRA fine‐tuning** or limited epochs so you’re feasible on simple hardware.
* Focus on showing the workflow: data generation, fine‐tuning, evaluation, improvement—not on beating SOTA.

---

## Simplified SEAL workflow you can implement

Here’s the simplified version of what you’ll code and run (we’ll translate this later into your project steps):

1. **Select your downstream task & data**

   * Example: Take short passages (say 50-100 words) on a topic you pick.
   * Create for each passage a small set of questions + answers (ground-truth) that test understanding.
   * This gives you your “context” (C) and “task evaluation” (τ).

2. **Baseline model performance**

   * Load your small LM (e.g., `distilgpt2` or something similar).
   * Fine-tune it (or just use it) on the initial dataset (maybe none, maybe minimal) and measure how well it answers the questions without the new info or without self-edits.
   * Record baseline accuracy or metric.

3. **Generate “self-edits”**

   * For each passage/context C, ask the model (via prompt) to generate:
     a) Synthetic training examples (e.g., paraphrase the passage into simpler statements, or generate QA pairs)
     b) A small “instruction” directive e.g. “train for 3 epochs, learning rate = 5e-5”
   * Save these self‐edits.

4. **Fine-tune with self‐edits**

   * Use the synthetic data generated as training data. Fine‐tune the model (or adapter) according to the directive (or use your default directive if hardware doesn’t allow flexibility).
   * After training, evaluate the model on the original evaluation questions (τ) without giving it the passage again.
   * Measure the performance improvement over baseline.

5. **Select best self‐edit(s)**

   * If you generated several self‐edit variants for each passage, you can pick the variant that gave the largest improvement.
   * You may not implement full RL policy update, but you can *simulate* by selecting best performing edits and discarding poor ones.

6. **(Optional) Second round**

   * Using the best self‐edits, repeat steps 3-5 with maybe different directives or more data.
   * Record improvement again.

7. **Compare & Document**

   * Compare baseline vs after self‐edits.
   * Provide concrete examples of the synthetic data your model generated.
   * Reflect on which types of edits worked better (e.g., simpler statements vs QA pairs, more epochs vs fewer epochs).
   * Note limitations (hardware, number of rounds, forgetting risk).


[1]: https://arxiv.org/html/2506.10943v1?utm_source=chatgpt.com "Self-Adapting Language Models - arXiv"
[2]: https://jyopari.github.io/posts/seal?utm_source=chatgpt.com "Self-Adapting Language Models - Jyothish Pari"
[3]: https://arxiv.org/abs/2506.10943?utm_source=chatgpt.com "Self-Adapting Language Models"
[4]: https://syncedreview.com/2025/06/16/mit-researchers-unveil-seal-a-new-step-towards-self-improving-ai/?utm_source=chatgpt.com "MIT Researchers Unveil “SEAL”: A New Step Towards Self ..."
[5]: https://www.researchgate.net/publication/392629858_Self-Adapting_Language_Models?utm_source=chatgpt.com "(PDF) Self-Adapting Language Models - ResearchGate"

Here are the core ideas of the full SEAL method, phrased plainly:

1. You have a base language model (LM) trained on lots of general data. ([arXiv][1])
2. When the model sees *new information* (for example: a new text passage it has not seen before), it doesn’t just passively store it or rely on in-context only; instead the model **writes its own training examples** (synthetic data) and also **writes instructions** (like: how many epochs to train, what learning rate) for how to fine-tune itself. These are called “self-edits”. ([jyopari.github.io][2])
3. The model then uses those self-edits to fine‐tune itself (i.e., update its weights) so the information becomes “internalised” rather than just in the prompt. ([arXiv][3])
4. After that fine-tuning, the model is tested on the downstream task (for example: answer questions about the new info without seeing the original passage). The performance gives a reward signal. If the performance improved, the “self-edit” strategy is deemed good. ([jyopari.github.io][2])
5. Using that reward, there is a meta‐learning / reinforcement learning (RL) outer loop: the model learns *how* to write better self‐edits and instructions over time (generate data + update strategy that yield better results). ([arXiv][1])
6. Repeat: new data → self‐edit → fine‐tune → evaluate → update edit-policy → new data, etc. This yields a model that *can adapt itself*, not just respond via prompt but actually change its weights. ([syncedreview.com][4])

**Why this is cool**: It mimics how a human student might learn: sees new info → writes notes or practice questions → studies them → tests themselves → improves how they study next time. The model is doing analogous steps. ([ResearchGate][5])

**Why it’s hard**:

* You need a credible downstream task with ground truth for evaluation (so you can compute reward).
* You need enough resources to fine-tune the model, generate many self-edits, evaluate repeatedly (which is compute‐heavy).
* There is risk of “forgetting” old info (catastrophic forgetting) when new updates overwrite old knowledge. ([arXiv][3])
* The full RL outer loop is complex; you may simplify.

---

## How you can *simplify* SEAL for your hardware & skills

Since you’re a student with a simple computer (less powerful GPU or maybe CPU), you should scale this down. Here’s how to adapt SEAL to be feasible:

* Use a **small language model** rather than huge LLMs (reduces compute, memory).
* Use **only one or two rounds** of “self-edit → fine‐tune → evaluate”, rather than many outer RL loops.
* Skip or heavily simplify the RL component: instead of a full policy gradient or PPO, pick the best self‐edit out of a small set, or manually analyse which edits improve performance; you don’t need to implement complex RL.
* Use a **small downstream task**: e.g., a short passage plus a few questions, or a classification task, rather than large benchmarks.
* Use **adapter/LoRA fine‐tuning** or limited epochs so you’re feasible on simple hardware.
* Focus on showing the workflow: data generation, fine‐tuning, evaluation, improvement—not on beating SOTA.

---

## Simplified SEAL workflow you can implement

Here’s the simplified version of what you’ll code and run (we’ll translate this later into your project steps):

1. **Select your downstream task & data**

   * Example: Take short passages (say 50-100 words) on a topic you pick.
   * Create for each passage a small set of questions + answers (ground-truth) that test understanding.
   * This gives you your “context” (C) and “task evaluation” (τ).

2. **Baseline model performance**

   * Load your small LM (e.g., `distilgpt2` or something similar).
   * Fine-tune it (or just use it) on the initial dataset (maybe none, maybe minimal) and measure how well it answers the questions without the new info or without self-edits.
   * Record baseline accuracy or metric.

3. **Generate “self-edits”**

   * For each passage/context C, ask the model (via prompt) to generate:
     a) Synthetic training examples (e.g., paraphrase the passage into simpler statements, or generate QA pairs)
     b) A small “instruction” directive e.g. “train for 3 epochs, learning rate = 5e-5”
   * Save these self‐edits.

4. **Fine-tune with self‐edits**

   * Use the synthetic data generated as training data. Fine‐tune the model (or adapter) according to the directive (or use your default directive if hardware doesn’t allow flexibility).
   * After training, evaluate the model on the original evaluation questions (τ) without giving it the passage again.
   * Measure the performance improvement over baseline.

5. **Select best self‐edit(s)**

   * If you generated several self‐edit variants for each passage, you can pick the variant that gave the largest improvement.
   * You may not implement full RL policy update, but you can *simulate* by selecting best performing edits and discarding poor ones.

6. **(Optional) Second round**

   * Using the best self‐edits, repeat steps 3-5 with maybe different directives or more data.
   * Record improvement again.

7. **Compare & Document**

   * Compare baseline vs after self‐edits.
   * Provide concrete examples of the synthetic data your model generated.
   * Reflect on which types of edits worked better (e.g., simpler statements vs QA pairs, more epochs vs fewer epochs).
   * Note limitations (hardware, number of rounds, forgetting risk).


[1]: https://arxiv.org/html/2506.10943v1?utm_source=chatgpt.com "Self-Adapting Language Models - arXiv"
[2]: https://jyopari.github.io/posts/seal?utm_source=chatgpt.com "Self-Adapting Language Models - Jyothish Pari"
[3]: https://arxiv.org/abs/2506.10943?utm_source=chatgpt.com "Self-Adapting Language Models"
[4]: https://syncedreview.com/2025/06/16/mit-researchers-unveil-seal-a-new-step-towards-self-improving-ai/?utm_source=chatgpt.com "MIT Researchers Unveil “SEAL”: A New Step Towards Self ..."
[5]: https://www.researchgate.net/publication/392629858_Self-Adapting_Language_Models?utm_source=chatgpt.com "(PDF) Self-Adapting Language Models - ResearchGate"
