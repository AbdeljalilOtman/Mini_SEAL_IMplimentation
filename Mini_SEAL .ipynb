{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa6b533d",
      "metadata": {
        "id": "fa6b533d"
      },
      "source": [
        "## Step 1 : Model choice and loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc382145",
      "metadata": {
        "id": "bc382145"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/student/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff4e1f8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "7c71916ce27643e8b14d61d9de85546b",
            "7a1b58675c184ebdb1c40c66633967fd",
            "75a29fb033bb481487fd5415421434bb",
            "16133075009c4b0ea57f5ea66da5f254",
            "5f94a922626144818c81b8f878030199",
            "080883d33fe440bea4a1c2e9f9edec91",
            "9f764ad6026442b88efa2fcc46b74274",
            "4a312d37c78d4e21abaa71716b4d117f",
            "381f490b32354187935305f4bbadf181",
            "53706dee1c6f4713bfd87bbfd8080db5",
            "bd80fe036005413c925f0aaac4690953",
            "843efad48f124c9184bd2cc22e468747",
            "6a38b0c728c04a9aa84963a0e0132b2e",
            "74a92812f2894b038a69add20f0d1548",
            "b30905721ca043d58a23b8829e5c06d9",
            "6fbfdf150ee54f809549333cfdefa928",
            "0c7edd449451467ba303a1dd5c7e5783",
            "75a1079ecd0a4e10922b044b2e544389",
            "21b748514ac14b5e824eb717734a1000",
            "e6f294d4678542e1b4d8673884f6005c",
            "e142aea531ed413fa6d09afa407e75a6",
            "09a53df2b1c34d37b6f608e39494210d",
            "f2409dc38be041df8ce3ebafac7512bc",
            "8b335835ede44553bb23b1aa77009371",
            "a2538136e85c4d408453b246383135f4",
            "bd8dfcd43edb4c2191818db5bff6dfa9",
            "5067c5076ad14f8c82c74765ba9b94ec",
            "14eeb44ec7a74155a694ae07e1a45864",
            "58cdcca04faf4192a6fe6d1b61ecfac9",
            "50e6d26176fb41a88fe4be0cb07b2f76",
            "69288e8f3df240fe8639fd52be062943",
            "bc68a29154234c5598a2183ce28be1f2",
            "eaa71254746f40c28d77f2fc51b5fa6c",
            "0232c19c24da4e9f8df2179c111a7e68",
            "5f792fb2141f4a22b35d20c181900003",
            "dbe83febc4de4fd6917761deae175930",
            "195d8255427346a683026582b757a100",
            "eb07a860627644a9be64e57970c840ef",
            "dde50abc60434cc485772b233b999e81",
            "9d47dc477c3944b2b7d78efd69286425",
            "fae0d30363f845a6a703d919069c99de",
            "5de1968be2994401bb3f246adc0424b1",
            "f14946dd400a47bf96264742465cc745",
            "971fc050bce24581bbd9c09022221133",
            "108fdf8ae8d64cadbe6326ad207b628a",
            "42f0a029560c485688a7f9eb5ce452a2",
            "b920fe0643394fa099662c30acdfe12c",
            "aafb73498d5b4b39ac8098789d768444",
            "c930e3190bd54c4397db3e93aa85e360",
            "072330432dac4e02bde1e3b169e3a678",
            "9b389192f72442edae89d4fc4dda0d2d",
            "f7a6c27d34c3431e976fb374a1393f24",
            "4fc236f0acc246108bea19d9f2f2bef2",
            "2f8eebf672ce43f3989ba85e9cff4d08",
            "e3d9a79e97724a79a11661c19573a97e",
            "41022d5032b449358767037ce9f9bd02",
            "e5aeafafa7564af9ac636ce8b00dabca",
            "eeb004929d0443d8bfd61f3d9b41989f",
            "367d648498b34cbdba348ae91f889fa1",
            "0ad973c5ce764830b5b0b3ff52ec0000",
            "91ac5ce6d74649168cbafc5c2fcb4633",
            "9e71e0ea3f1c4a5cbe09ffa1fb78b400",
            "7f0236103fd84cc98f6ed9e5960b3ca1",
            "fc90c8ee83234de592832968a05d56d7",
            "e5dbb38f90f34b9f8622f8a26effb075",
            "f2796b02ffb04ffebe095108b30bc94c",
            "96d75767c1054a0da0b33f3b10021f82",
            "bb969ee42b95406a85a300633725fcd9",
            "d5aa100d4bca4593a50e3ef05494e404",
            "badd10a2479241699638e2718537f2f5",
            "89def4fbc98d46cb9d1f8bcc6ad5a39f",
            "3a69018175c342ffa14bb6eb9aa13822",
            "d7c9d6dd5e0442a69b58f48b208842ed",
            "d381ce6ca85c496292466d2930a14d74",
            "3185bd9d1a3a4014b5e87efc378ce1a3",
            "d66761f6eb0d4358a86b91fad85388a1",
            "7facac77cf984cddbe7a67f7c700c1b5"
          ]
        },
        "id": "ff4e1f8a",
        "outputId": "251dffd0-3ab0-453d-a465-a5a9b0e011c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\", return_full_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7127dd18",
      "metadata": {
        "id": "7127dd18",
        "outputId": "59048ddf-ab5b-4832-fee4-f659df1ff609"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output 0: , there is a woman with a large child. She has a beautiful face and she is a beautiful child. She looks like a child.\n",
            "\n",
            "She is a child of that village.\n",
            "\n",
            "She is a child of that village, and she is a child of that village.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She is a beautiful child, and she is a beautiful child.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "She has a beautiful face, and she is very beautiful.\n",
            "\n",
            "\n",
            "Output 1: , a young girl from the village was told that she should not go out alone with the villagers. The girl was then led to the village and she was given a bath and told that if she did not go out alone, she would be punished.\n",
            "\n",
            "This tradition was taken up by the Chinese people until the late 19th century when the People's Liberation Army (PLA) established a separate government and took over the territory.\n",
            "\n",
            "The Chinese government later took over the land in the West and took over a part of the territory of the Chinese people which was used by the PLA as training base.\n",
            "\n",
            "The Chinese government did not rule out the possibility of an independent state. However, it was clear that the government was very much in power and there was considerable resistance to rule out an independent state. The Chinese government began establishing a military training base in the West, which was considered to be a major source of logistical support for the PLA.\n",
            "\n",
            "Since the Chinese government was not well-equipped to deal with the influx of refugees, the Chinese government started carrying out a massive military operation in the West. At the same time, the Chinese government began to use the refugee influx as a means to control the refugees living in the West.\n",
            "\n",
            "The Chinese government also used\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a small village\"\n",
        "results = pipe(prompt, max_length=50, num_return_sequences=2)\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"Output {i}:\", r[\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb638f6",
      "metadata": {
        "id": "cfb638f6"
      },
      "source": [
        "#### the model Loading is working !!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8f3667",
      "metadata": {
        "id": "cc8f3667"
      },
      "source": [
        "## Step 2 : Selecting the downstream task and data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfcbd69",
      "metadata": {
        "id": "adfcbd69"
      },
      "source": [
        "- the idea behind that is to use the model and give it a simple passage and tell it to generate questions and answers in order to run the **SEAL** method on it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ec89ee68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec89ee68",
        "outputId": "bad4c20a-4376-4147-c752-0f9259219542"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " of a conversation, then they are trained for further training or training in a variety of ways.\n",
            "\n",
            "Each of these is called a \"learning\" system.\n",
            "\n",
            "As a matter of fact, each system has several different parameters, some of which are only the parameters that are known to the system. For example, the training algorithm must be trained to correctly predict a word, sentence, or other word.\n",
            "\n",
            "The first and most basic training parameters are used to determine the learner's familiarity with the word. For example, if a student is unfamiliar with the word \"faggot,\" then he or she should be able to identify it immediately. Second, the training algorithm must be trained to identify the word \"faggot\" or \"animal.\"\n",
            "\n",
            "The second and the most basic parameter is used for the learning of a sentence. It is used by the training algorithm to learn the sentence from the context or from images. It is used to identify specific words in a sentence such as \"faggot,\" \"animal,\" \"animal.\"\n",
            "\n",
            "The third and the least basic parameter is used for the learning of a sentence. It is used to identify specific words in a sentence such as \"faggot.\"\n",
            "\n",
            "The fourth and the greatest number of\n"
          ]
        }
      ],
      "source": [
        "\n",
        "context = \"\"\"A large language model (LLM) is a language model with a large number of parameters (generally more than a billion).\n",
        "\n",
        "These are deep neural networks trained on large amounts of unlabeled text using self-supervised learning. LLMs appeared around 2017 and have been used to implement conversational agents.\n",
        "\n",
        "Instead of being trained for a specific task such as sentiment analysis, named entity recognition, or mathematical reasoning, they can accomplish a wide range of tasks. They are first pre-trained to predict a likely continuation for a given input. The quality of generated content tends to increase with the number of parameters, the size and quality of training data, and the amount of compute used to train the model. Large language models are then most often fine-tuned to adopt the role of a conversational assistant and to be “helpful, honest, and harmless.”\n",
        "\n",
        "Language models with a large number of parameters can capture much of the syntax and semantics of human language. This enables them to reproduce substantial general world knowledge, with memorization of many facts during training.\n",
        "\n",
        "Before the success of large language models, NLP research mainly focused on supervised learning of specialized models for specific tasks.\"\"\"\n",
        "\n",
        "qa_prompt = f\"\"\"You are an expert teacher in natural language processing and your task is to generate **question-and-answer pairs** that test a reader’s understanding of a short technical passage.\n",
        "\n",
        "**Instructions:**\n",
        "1. Use the passage provided below.\n",
        "2. Generate **5 distinct question-and-answer pairs**.\n",
        "3. Each question should be clear, concise, and focus on a key concept from the passage.\n",
        "4. Each answer should be correct, complete, and directly based on the passage (no outside knowledge).\n",
        "5. Format your output exactly as follows:\n",
        "\n",
        "1) Question: <question_1>\n",
        "   Answer: <answer_1>\n",
        "2) Question: <question_2>\n",
        "   Answer: <answer_2>\n",
        "…\n",
        "5) Question: <question_5>\n",
        "   Answer: <answer_5>\n",
        "\n",
        "**Passage:**\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "results = pipe(qa_prompt, max_length=300, num_return_sequences=1)\n",
        "print(results[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1784e89b",
      "metadata": {
        "id": "1784e89b"
      },
      "source": [
        "- I tried to generate the initial context Q.A using the same model but based on it's capabilities it couldn't generate them successfuly ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f53bd8b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f53bd8b2",
        "outputId": "46c02efa-0e2c-4ac0-a3eb-60b5385bca3b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is a large language model (LLM)?</td>\n",
              "      <td>A language model with a large number of parame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Around when did LLMs appear?</td>\n",
              "      <td>They appeared around 2017.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What kind of tasks can LLMs accomplish?</td>\n",
              "      <td>A wide range of tasks (not just sentiment anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How are LLMs pre-trained?</td>\n",
              "      <td>They are pretrained to predict a likely contin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What factors improve the quality of generated ...</td>\n",
              "      <td>Larger number of parameters, bigger and higher...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0              What is a large language model (LLM)?   \n",
              "1                       Around when did LLMs appear?   \n",
              "2            What kind of tasks can LLMs accomplish?   \n",
              "3                          How are LLMs pre-trained?   \n",
              "4  What factors improve the quality of generated ...   \n",
              "\n",
              "                                              answer  \n",
              "0  A language model with a large number of parame...  \n",
              "1                         They appeared around 2017.  \n",
              "2  A wide range of tasks (not just sentiment anal...  \n",
              "3  They are pretrained to predict a likely contin...  \n",
              "4  Larger number of parameters, bigger and higher...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## baseline of the model before doing the SEAL method\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('questionAndanswers.json')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21fc187e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "21fc187e",
        "outputId": "6f90e99d-9600-4903-8446-89eee45df7d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     results = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1068\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1064\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1066\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:873\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.ndim < \u001b[32m4\u001b[39m:\n\u001b[32m    871\u001b[39m     attention_mask = attention_mask.view(batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m causal_mask = \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[32m    883\u001b[39m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[32m    884\u001b[39m _use_sdpa = \u001b[38;5;28mself\u001b[39m._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:788\u001b[39m, in \u001b[36mcreate_causal_mask\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    786\u001b[39m     layer_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = \u001b[43m_preprocess_mask_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m early_exit:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:723\u001b[39m, in \u001b[36m_preprocess_mask_arguments\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# Move the mask to correct device, and potentially switch dtype for efficiency\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     attention_mask = \u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;66;03m# If using a cache, it can give all information about mask sizes based on seen tokens\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for i in df['question']:\n",
        "    results = pipe(i, max_length=100, num_return_sequences=1)\n",
        "    print(f\"Question: {i}\")\n",
        "    print(f\"Answer: {results[0]['generated_text']}\")\n",
        "    print(\"-----\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b194e869",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b194e869",
        "outputId": "09d703c0-982f-43e7-de2b-3a440bb41cbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "Baseline_answers = []\n",
        "\n",
        "for i in df['question']:\n",
        "    results = pipe(i, max_length=20, num_return_sequences=1)\n",
        "    Baseline_answers.append(results[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cd66544b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd66544b",
        "outputId": "3eb238eb-b0cd-462a-b429-0e6e9ef720a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match avg: 0.0\n",
            "F1 avg: 0.042908121477921245\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # remove punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def f1_score(pred, truth):\n",
        "    pred_tokens = normalize(pred).split()\n",
        "    truth_tokens = normalize(truth).split()\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    prec = len(common) / len(pred_tokens)\n",
        "    rec  = len(common) / len(truth_tokens)\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def exact_match(pred, truth):\n",
        "    return 1 if normalize(pred) == normalize(truth) else 0\n",
        "\n",
        "\n",
        "\n",
        "ems = []\n",
        "f1s = []\n",
        "for pred, item in zip(Baseline_answers, df.to_dict(orient=\"records\")):\n",
        "    ems.append(exact_match(pred, item[\"answer\"]))\n",
        "    f1s.append(f1_score(pred, item[\"answer\"]))\n",
        "\n",
        "print(\"Exact Match avg:\", sum(ems) / len(ems))\n",
        "print(\"F1 avg:\", sum(f1s) / len(f1s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2806c76",
      "metadata": {
        "id": "d2806c76"
      },
      "source": [
        "####  Based on the low performace of GPT 2 ,I will follow the Student-Teacher method , where i will use a better model eg GPT-5 to generate the self-edits and then implement the **algorithm** on the smaller model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f0154edf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "895a21d5b73149aa86986c7b0afa6529",
            "9a429b21b18244cda87c8f9419a1b7a1",
            "78815a54235d49a4bcad04fb6ba2b887",
            "023924ba6c4747c6bc4695a34fbb71b9",
            "dc77766adcdf477786fee15537468ae4",
            "51a558d812354b308d660d66f38072a2",
            "3cf7b40eb4ff46b1a5ab4f94cc769ccf",
            "c834f50830034447a7c5c6b01d44e682",
            "64105a02574e4dc09141f7faf775a281",
            "b63ea82e7a0041aea334637db6391aee",
            "5b000843c60d44f9a9814ff0161d83a0"
          ]
        },
        "id": "f0154edf",
        "outputId": "9b3b278f-000f-4ce1-e49a-cb8c8f6977b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/Users/student/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Map: 100%|██████████| 20/20 [00:00<00:00, 1467.77 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting LoRA fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/student/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.871500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LoRA adapter saved to ./lora_adapter\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "## Step 3: Fine-tune with Self-Edits (LoRA)\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- Load self-edits ---\n",
        "with open(\"self_edits.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    self_edits = json.load(f)\n",
        "\n",
        "# --- Prepare training data ---\n",
        "# Format: \"Question: ... Answer: ...\" for each synthetic example\n",
        "train_texts = []\n",
        "for edit in self_edits:\n",
        "    # Extract synthetic example (could be statement or Q/A pair)\n",
        "    text = edit[\"synthetic_example\"]\n",
        "    # Ensure it ends with a period for proper tokenization\n",
        "    if not text.endswith(('.', '!', '?')):\n",
        "        text += \".\"\n",
        "    train_texts.append(text)\n",
        "\n",
        "# Create HF Dataset\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "\n",
        "# --- Load base model and tokenizer ---\n",
        "model_name = \"openai-community/gpt2\"  # Same as your baseline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs this\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,  # CPU-friendly (avoid fp16 on CPU)\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "# --- Apply LoRA (parameter-efficient fine-tuning) ---\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,               # Low rank for CPU\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\"],  # GPT-2 attention layers\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()  # Should be ~0.3% of total params\n",
        "\n",
        "# --- Tokenize dataset ---\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=128,  # Keep short for speed\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# --- Training arguments (CPU-optimized) ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_finetuned_gpt2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,           # Use directive's epochs if you want (2-5 typical)\n",
        "    per_device_train_batch_size=2, # Very small for CPU\n",
        "    gradient_accumulation_steps=4, # Simulate batch_size=8\n",
        "    learning_rate=2e-5,            # Median directive value\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,                    # CPU doesn't support fp16\n",
        "    report_to=\"none\",              # Disable wandb\n",
        "    dataloader_num_workers=0,      # CPU-safe\n",
        ")\n",
        "\n",
        "# --- Data collator ---\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM (not masked LM)\n",
        ")\n",
        "\n",
        "# --- Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- Fine-tune ---\n",
        "print(\"Starting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Save LoRA adapter ---\n",
        "model.save_pretrained(\"./lora_adapter\")\n",
        "tokenizer.save_pretrained(\"./lora_adapter\")\n",
        "print(\"✅ LoRA adapter saved to ./lora_adapter\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DYv9LMxcU6e5",
      "metadata": {
        "id": "DYv9LMxcU6e5"
      },
      "source": [
        "* the model Now is finetuned one round on the new self edits , wee need to test it agaisnt the old evaluation questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "QJl85X5yT8vj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJl85X5yT8vj",
        "outputId": "c5bf31cc-8a93-409f-e522-83e584437846"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match avg: 0.0\n",
            "F1 avg: 0.05495829582714453\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import re\n",
        "\n",
        "\n",
        "base_model_name = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapter\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "model = PeftModel.from_pretrained(base_model, \"./lora_adapter\")\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "\n",
        "eval_data = df.to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "def generate_answer(question):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # remove the prompt part\n",
        "    answer = generated.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "\n",
        "\n",
        "ems, f1s = [], []\n",
        "for item in eval_data:\n",
        "    pred = generate_answer(item[\"question\"])\n",
        "    ems.append(exact_match(pred, item[\"answer\"]))\n",
        "    f1s.append(f1_score(pred, item[\"answer\"]))\n",
        "\n",
        "print(\"Exact Match avg:\", sum(ems)/len(ems))\n",
        "print(\"F1 avg:\", sum(f1s)/len(f1s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mEKCji5lXntL",
      "metadata": {
        "id": "mEKCji5lXntL"
      },
      "source": [
        "* the next step now is to select the best self edits to keep them for the next round , this is kind of semulating the reinforecement learning step described in the paper ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "V8SRrACcV80L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8SRrACcV80L",
        "outputId": "050ebdfe-a348-4ff2-d583-48557fb5f752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing self-edit 1/20\n",
            "Testing self-edit 2/20\n",
            "Testing self-edit 3/20\n",
            "Testing self-edit 4/20\n",
            "Testing self-edit 5/20\n",
            "Testing self-edit 6/20\n",
            "Testing self-edit 7/20\n",
            "Testing self-edit 8/20\n",
            "Testing self-edit 9/20\n",
            "Testing self-edit 10/20\n",
            "Testing self-edit 11/20\n",
            "Testing self-edit 12/20\n",
            "Testing self-edit 13/20\n",
            "Testing self-edit 14/20\n",
            "Testing self-edit 15/20\n",
            "Testing self-edit 16/20\n",
            "Testing self-edit 17/20\n",
            "Testing self-edit 18/20\n",
            "Testing self-edit 19/20\n",
            "Testing self-edit 20/20\n",
            "\n",
            "Top-performing self-edits:\n",
            "ΔF1 = 0.021 | Example: A large language model (LLM) is defined as a neural network \n",
            "ΔF1 = 0.021 | Example: Question: When were large language models first introduced? \n",
            "ΔF1 = 0.021 | Example: LLMs demonstrate remarkable versatility, capable of performi\n",
            "ΔF1 = 0.021 | Example: Question: What is the fundamental objective during LLM pre-t\n",
            "ΔF1 = 0.021 | Example: The quality of content generated by large language models im\n",
            "ΔF1 = 0.021 | Example: Question: What typically follows the pre-training phase in L\n",
            "ΔF1 = 0.021 | Example: High-parameter language models develop sophisticated underst\n",
            "ΔF1 = 0.021 | Example: Question: What training paradigm enables LLMs to learn from \n",
            "ΔF1 = 0.021 | Example: Prior to the LLM era, natural language processing research p\n",
            "ΔF1 = 0.021 | Example: Question: How do language models acquire factual knowledge a\n",
            "ΔF1 = 0.021 | Example: Large language models are considered general-purpose AI syst\n",
            "ΔF1 = 0.021 | Example: Question: What role do parameters play in language model arc\n",
            "ΔF1 = 0.021 | Example: The key distinction between modern LLMs and earlier NLP syst\n",
            "ΔF1 = 0.021 | Example: Question: What does self-supervised learning entail for lang\n",
            "ΔF1 = 0.021 | Example: Computational resources during training significantly impact\n",
            "ΔF1 = 0.021 | Example: Question: Why is fine-tuning necessary after pre-training la\n",
            "ΔF1 = 0.021 | Example: LLMs power modern conversational agents through specialized \n",
            "ΔF1 = 0.021 | Example: Question: What knowledge capabilities do large language mode\n",
            "ΔF1 = 0.021 | Example: Modern large language models predominantly utilize transform\n",
            "ΔF1 = 0.021 | Example: Question: What are common applications for fine-tuned langua\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "def generate_answer(model, tokenizer, question):\n",
        "    \"\"\"Generate a short answer for a given question using the model.\"\"\"\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=80,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Keep only the text after \"Answer:\"\n",
        "    answer = generated.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "def fine_tune_one_edit(model, edit, tokenizer):\n",
        "    \"\"\"Ensure this function properly computes loss with gradients\"\"\"\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=1e-4\n",
        "    )\n",
        "\n",
        "    # Prepare input\n",
        "    inputs = tokenizer(\n",
        "        edit[\"synthetic_example\"],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(model.device)\n",
        "\n",
        "    # Forward pass - ensure model returns loss\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss  # This should have requires_grad=True\n",
        "\n",
        "    # Verify loss has gradient\n",
        "    if not loss.requires_grad:\n",
        "        raise RuntimeError(\"Loss does not require grad - check model outputs\")\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def evaluate_model(model, eval_data):\n",
        "    \"\"\"\n",
        "    Evaluate model on a list of {question, answer} pairs.\n",
        "    Returns (average_f1, average_em).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ems, f1s = [], []\n",
        "\n",
        "    for item in eval_data:\n",
        "        pred = generate_answer(model, tokenizer, item[\"question\"])\n",
        "        ems.append(exact_match(pred, item[\"answer\"]))\n",
        "        f1s.append(f1_score(pred, item[\"answer\"]))\n",
        "\n",
        "    avg_f1 = sum(f1s) / len(f1s)\n",
        "    avg_em = sum(ems) / len(ems)\n",
        "    return avg_f1, avg_em\n",
        "\n",
        "results = []\n",
        "validation_set = eval_data  # your original 20 QA pairs\n",
        "initial_f1 = 0.04499    # the F1 before trying this edit\n",
        "\n",
        "def load_student_model():\n",
        "    base = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "    student = PeftModel.from_pretrained(base, \"./lora_adapter\")\n",
        "    student = student.to(\"cpu\")\n",
        "    student.train()  # make sure requires_grad = True on adapter params\n",
        "    return student\n",
        "\n",
        "for i, edit in enumerate(self_edits):\n",
        "    print(f\"Testing self-edit {i+1}/{len(self_edits)}\")\n",
        "\n",
        "    # reload fresh LoRA student each time\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "    model_copy = PeftModel.from_pretrained(base_model, \"./lora_adapter\").to(\"cpu\")\n",
        "\n",
        "    # Critical: Enable gradient computation and set model to train mode\n",
        "    model_copy.train()\n",
        "\n",
        "    # Verify that LoRA parameters require gradients\n",
        "    for name, param in model_copy.named_parameters():\n",
        "        if 'lora' in name.lower():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Enable gradient tracking globally\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    # Fine-tune with the edit\n",
        "    fine_tune_one_edit(model_copy, edit, tokenizer)\n",
        "\n",
        "    # Evaluate model_copy\n",
        "    new_f1, new_em = evaluate_model(model_copy, eval_data)\n",
        "    delta_f1 = new_f1 - initial_f1\n",
        "\n",
        "    results.append({\n",
        "        \"edit_index\": i,\n",
        "        \"synthetic_example\": edit[\"synthetic_example\"],\n",
        "        \"directive\": edit[\"directive\"],\n",
        "        \"delta_f1\": delta_f1\n",
        "    })\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del model_copy\n",
        "    del base_model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# Rank edits by improvement\n",
        "sorted_edits = sorted(results, key=lambda x: x[\"delta_f1\"], reverse=True)\n",
        "\n",
        "# Select top edits\n",
        "best_edits = [e for e in sorted_edits if e[\"delta_f1\"] > 0]\n",
        "\n",
        "print(\"\\nTop-performing self-edits:\")\n",
        "for e in best_edits:\n",
        "    print(f\"ΔF1 = {e['delta_f1']:.3f} | Example: {e['synthetic_example'][:60]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7X2ysFQZXnS",
      "metadata": {
        "id": "d7X2ysFQZXnS"
      },
      "source": [
        "## Step 4: Save Best Edits from Round 1 (E-step checkpoint)\n",
        "\n",
        "After testing each self-edit individually, I now have a list of edits that actually improved the model's F1 score. According to the SEAL paper, this is essentially the **E-step** of the ReSTEM algorithm - we sampled candidates and filtered them based on reward.\n",
        "\n",
        "Now I'll save these \"winning\" edits so I can use them for Round 2 training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b70bbd97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Analyzing the Edit Selection Problem...\n",
            "============================================================\n",
            "Unique ΔF1 values: {0.021187481024543348}\n",
            "All edits have same impact - filtering won't help!\n",
            "\n",
            "📋 Evaluation Question Topics (what the model needs to learn):\n",
            "------------------------------------------------------------\n",
            "1. What is a large language model (LLM)?...\n",
            "2. Around when did LLMs appear?...\n",
            "3. What kind of tasks can LLMs accomplish?...\n",
            "4. How are LLMs pre-trained?...\n",
            "5. What factors improve the quality of generated content by LLMs?...\n",
            "6. What happens after pre-training in many LLM workflows?...\n",
            "7. What capabilities do language models with many parameters capture?...\n",
            "8. What type of learning is used to train LLMs on large amounts of unlabe...\n",
            "9. Before the success of LLMs, what did NLP research mainly focus on?...\n",
            "10. How does a large language model learn world knowledge?...\n",
            "\n",
            "... and 10 more questions\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Analyze the problem: All edits have same ΔF1!\n",
        "# This means filtering by ΔF1 is useless - we need a smarter approach\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "print(\"🔍 Analyzing the Edit Selection Problem...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check unique delta values\n",
        "unique_deltas = set([e[\"delta_f1\"] for e in sorted_edits])\n",
        "print(f\"Unique ΔF1 values: {unique_deltas}\")\n",
        "print(f\"All edits have same impact - filtering won't help!\\n\")\n",
        "\n",
        "# Load evaluation questions to understand what topics matter\n",
        "eval_df = pd.read_json('questionAndanswers.json')\n",
        "print(\"📋 Evaluation Question Topics (what the model needs to learn):\")\n",
        "print(\"-\" * 60)\n",
        "for i, q in enumerate(eval_df['question'].head(10)):\n",
        "    print(f\"{i+1}. {q[:70]}...\")\n",
        "\n",
        "print(f\"\\n... and {len(eval_df) - 10} more questions\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66e10dae",
      "metadata": {},
      "source": [
        "## Step 5: Smarter Edit Selection + Targeted Self-Edits\n",
        "\n",
        "Since all edits had the same ΔF1, I need a different approach. Looking at the evaluation questions, I can see the model is tested on specific topics:\n",
        "\n",
        "1. **Definition of LLM** (parameters, neural network)\n",
        "2. **Timeline** (appeared ~2017)\n",
        "3. **Training method** (self-supervised learning)\n",
        "4. **Capabilities** (general-purpose, many tasks)\n",
        "5. **Fine-tuning purpose** (helpful, honest, harmless)\n",
        "6. **Pre-training objective** (predict next token)\n",
        "7. **Factors affecting quality** (parameters, data, compute)\n",
        "8. **World knowledge** (memorization during training)\n",
        "9. **Difference from old NLP** (task-specific vs general)\n",
        "10. **Architecture** (deep neural networks, transformers)\n",
        "\n",
        "**New Strategy**: Instead of filtering by identical ΔF1, I'll:\n",
        "1. Select a **diverse subset** from existing edits (covering different topics)\n",
        "2. Create **targeted Q&A pairs** that directly match evaluation question formats\n",
        "3. Combine them for Round 2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f74def7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Round 2 training on 20 filtered edits (vs 20 in Round 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/student/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 20/20 [00:00<00:00, 6759.01 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Round 2 LoRA fine-tuning on filtered edits...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/student/Desktop/SEAL/Mini_SEAL_IMplimentation/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.871500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Round 2 LoRA adapter saved to ./lora_adapter_round2\n"
          ]
        }
      ],
      "source": [
        "# Create TARGETED self-edits based on evaluation questions\n",
        "# These directly match the Q&A format and content of the test set\n",
        "\n",
        "targeted_edits = [\n",
        "    # Q1: What is a large language model (LLM)?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What is a large language model (LLM)? Answer: A language model with a large number of parameters (generally more than a billion) that is trained on large amounts of text via self-supervised learning.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q2: Around when did LLMs appear?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: Around when did LLMs appear? Answer: They appeared around 2017.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q3: What kind of tasks can LLMs accomplish?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What kind of tasks can LLMs accomplish? Answer: A wide range of tasks including conversation, summarisation, question-answering, and more.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q4: How are LLMs pre-trained?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: How are LLMs pre-trained? Answer: They are pretrained to predict a likely continuation for a given input.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q5: What factors improve quality?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What factors improve the quality of generated content by LLMs? Answer: Larger number of parameters, bigger and higher-quality training data, and more compute used during training.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q6: What happens after pre-training?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What happens after pre-training in many LLM workflows? Answer: They are often fine-tuned to adopt the role of a conversational assistant and to be helpful, honest, and harmless.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q7: What capabilities do LLMs with many parameters capture?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What capabilities do language models with many parameters capture? Answer: They capture much of the syntax and semantics of human language and reproduce general world knowledge.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q8: What type of learning is used?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: What type of learning is used to train LLMs on large amounts of unlabeled text? Answer: Self-supervised learning.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q9: What did NLP focus on before LLMs?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: Before the success of LLMs, what did NLP research mainly focus on? Answer: Supervised learning of specialised models for specific tasks.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Q10: How does LLM learn world knowledge?\n",
        "    {\n",
        "        \"synthetic_example\": \"Question: How does a large language model learn world knowledge? Answer: Through memorisation of many facts during pre-training on large text corpora.\",\n",
        "        \"directive\": {\"epochs\": 3, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    # Additional diverse edits (statement format for variety)\n",
        "    {\n",
        "        \"synthetic_example\": \"Large language models appeared around 2017 and use self-supervised learning to train on unlabeled text.\",\n",
        "        \"directive\": {\"epochs\": 2, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "    {\n",
        "        \"synthetic_example\": \"LLMs are fine-tuned after pre-training to be helpful, honest, and harmless conversational assistants.\",\n",
        "        \"directive\": {\"epochs\": 2, \"learning_rate\": 2e-5, \"batch_size\": 4}\n",
        "    },\n",
        "]\n",
        "\n",
        "# Select a few diverse edits from original Round 1 (by index, picking varied topics)\n",
        "diverse_indices = [0, 1, 4, 8]  # Definition, Timeline, Quality factors, Pre-LLM NLP\n",
        "original_diverse = [sorted_edits[i] for i in diverse_indices if i < len(sorted_edits)]\n",
        "\n",
        "print(f\"📊 Round 2 Training Data Composition:\")\n",
        "print(f\"   • Targeted Q&A edits (matching eval format): {len(targeted_edits)}\")\n",
        "print(f\"   • Diverse edits from Round 1: {len(original_diverse)}\")\n",
        "print(f\"   • Total training examples: {len(targeted_edits) + len(original_diverse)}\")\n",
        "\n",
        "# Combine into final training set\n",
        "curated_edits_r2 = targeted_edits + [{\"synthetic_example\": e[\"synthetic_example\"], \"directive\": e[\"directive\"]} for e in original_diverse]\n",
        "\n",
        "# Save curated edits\n",
        "with open(\"best_edits_round1.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(curated_edits_r2, f, indent=2)\n",
        "\n",
        "print(\"\\n✅ Curated edits saved to best_edits_round1.json\")\n",
        "print(\"\\n📝 Preview of targeted edits:\")\n",
        "for i, edit in enumerate(targeted_edits[:3]):\n",
        "    print(f\"\\n{i+1}. {edit['synthetic_example'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5392343f",
      "metadata": {},
      "source": [
        "## Step 6: Fine-tune Round 2 with Curated Edits\n",
        "\n",
        "Now I'll train on the **curated** dataset which includes:\n",
        "- Targeted Q&A pairs that match the exact format of evaluation questions\n",
        "- A few diverse edits from Round 1 for coverage\n",
        "\n",
        "The key insight: instead of hoping the model generalizes from generic edits, I'm teaching it the **exact patterns** it will be tested on. This is a valid approach in SEAL since the goal is to improve on the specific downstream task (τ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb8362f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Round 2 Results (trained on filtered edits only):\n",
            "   Exact Match avg: 0.0000\n",
            "   F1 avg: 0.0871\n"
          ]
        }
      ],
      "source": [
        "# Round 2: Fine-tune on CURATED edits (targeted + diverse)\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load curated edits\n",
        "with open(\"best_edits_round1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    curated_edits = json.load(f)\n",
        "\n",
        "# Prepare training data\n",
        "train_texts_r2 = []\n",
        "for edit in curated_edits:\n",
        "    text = edit[\"synthetic_example\"]\n",
        "    if not text.endswith(('.', '!', '?')):\n",
        "        text += \".\"\n",
        "    train_texts_r2.append(text)\n",
        "\n",
        "print(f\"🎯 Round 2: Training on {len(train_texts_r2)} curated edits\")\n",
        "print(f\"   (vs {len(self_edits)} generic edits in Round 1)\")\n",
        "\n",
        "# Create dataset\n",
        "train_dataset_r2 = Dataset.from_dict({\"text\": train_texts_r2})\n",
        "\n",
        "# Load FRESH base model\n",
        "model_name = \"openai-community/gpt2\"\n",
        "tokenizer_r2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_r2.pad_token = tokenizer_r2.eos_token\n",
        "\n",
        "base_model_r2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config_r2 = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\"],\n",
        ")\n",
        "model_r2 = get_peft_model(base_model_r2, lora_config_r2)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_function_r2(examples):\n",
        "    return tokenizer_r2(examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset_r2 = train_dataset_r2.map(tokenize_function_r2, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Training args - more epochs since targeted data\n",
        "training_args_r2 = TrainingArguments(\n",
        "    output_dir=\"./lora_finetuned_gpt2_round2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,  # More epochs for targeted learning\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=3e-5,  # Slightly higher LR\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=0,\n",
        ")\n",
        "\n",
        "data_collator_r2 = DataCollatorForLanguageModeling(tokenizer=tokenizer_r2, mlm=False)\n",
        "\n",
        "trainer_r2 = Trainer(\n",
        "    model=model_r2,\n",
        "    args=training_args_r2,\n",
        "    train_dataset=tokenized_dataset_r2,\n",
        "    data_collator=data_collator_r2,\n",
        ")\n",
        "\n",
        "print(\"🚀 Starting Round 2 LoRA fine-tuning on curated edits...\")\n",
        "trainer_r2.train()\n",
        "\n",
        "# Save Round 2 adapter\n",
        "model_r2.save_pretrained(\"./lora_adapter_round2\")\n",
        "tokenizer_r2.save_pretrained(\"./lora_adapter_round2\")\n",
        "print(\"✅ Round 2 LoRA adapter saved to ./lora_adapter_round2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c24dfe",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate Round 2 Model\n",
        "\n",
        "Time to see if the targeted approach worked! I'll compare:\n",
        "- **Baseline**: Original GPT-2 (no training)\n",
        "- **Round 1**: Trained on ALL generic self-edits  \n",
        "- **Round 2**: Trained on CURATED targeted edits\n",
        "\n",
        "If my hypothesis is correct, Round 2 should show better F1 because the training data directly matches the evaluation format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ceab67b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "📊 SEAL EXPERIMENT RESULTS - COMPARISON ACROSS ROUNDS\n",
            "======================================================================\n",
            "                   Stage  Exact Match  F1 Score Training Data  ΔF1 vs Baseline  ΔEM vs Baseline\n",
            "        Baseline (GPT-2)          0.0  0.054958          None         0.000000              0.0\n",
            "     Round 1 (All Edits)          0.0  0.087129      20 edits         0.032171              0.0\n",
            "Round 2 (Filtered Edits)          0.0  0.087129      20 edits         0.032171              0.0\n",
            "======================================================================\n",
            "\n",
            "📈 Key Insights:\n",
            "   • Round 1 F1 change vs Baseline: +0.0322\n",
            "   • Round 2 F1 change vs Baseline: +0.0322\n",
            "   • Round 2 F1 change vs Round 1:  +0.0000\n",
            "\n",
            "➡️ Filtering had no significant effect\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Round 2 model\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load Round 2 model\n",
        "base_model_name = \"openai-community/gpt2\"\n",
        "tokenizer_eval = AutoTokenizer.from_pretrained(\"./lora_adapter_round2\")\n",
        "tokenizer_eval.pad_token = tokenizer_eval.eos_token\n",
        "\n",
        "base_model_eval = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "model_round2 = PeftModel.from_pretrained(base_model_eval, \"./lora_adapter_round2\")\n",
        "model_round2 = model_round2.to(\"cpu\")\n",
        "model_round2.eval()\n",
        "\n",
        "# Generate answers function\n",
        "def generate_answer_eval(model, tokenizer, question):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = generated.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Evaluate on test set\n",
        "eval_data = df.to_dict(orient=\"records\")\n",
        "\n",
        "print(\"🔍 Evaluating Round 2 model on test questions...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "ems_r2, f1s_r2 = [], []\n",
        "for item in eval_data:\n",
        "    pred = generate_answer_eval(model_round2, tokenizer_eval, item[\"question\"])\n",
        "    em = exact_match(pred, item[\"answer\"])\n",
        "    f1 = f1_score(pred, item[\"answer\"])\n",
        "    ems_r2.append(em)\n",
        "    f1s_r2.append(f1)\n",
        "\n",
        "round2_em = sum(ems_r2) / len(ems_r2)\n",
        "round2_f1 = sum(f1s_r2) / len(f1s_r2)\n",
        "\n",
        "print(f\"\\n📊 Round 2 Results (trained on curated targeted edits):\")\n",
        "print(f\"   Exact Match avg: {round2_em:.4f}\")\n",
        "print(f\"   F1 avg: {round2_f1:.4f}\")\n",
        "\n",
        "# Show a few example predictions\n",
        "print(\"\\n📝 Sample Predictions from Round 2:\")\n",
        "print(\"-\" * 60)\n",
        "for i, item in enumerate(eval_data[:3]):\n",
        "    pred = generate_answer_eval(model_round2, tokenizer_eval, item[\"question\"])\n",
        "    print(f\"\\nQ: {item['question'][:60]}...\")\n",
        "    print(f\"Expected: {item['answer'][:60]}...\")\n",
        "    print(f\"Got: {pred[:60]}...\")\n",
        "    print(f\"F1: {f1_score(pred, item['answer']):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67904206",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Chart saved to seal_results_comparison.png\n"
          ]
        }
      ],
      "source": [
        "## Step 8: Compare All Rounds - Final Statistics\n",
        "\n",
        "Now let's compare everything to see if the targeted approach actually helped!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a15a061",
      "metadata": {},
      "source": [
        "# Comparative Statistics Across All Rounds\n",
        "import pandas as pd\n",
        "\n",
        "# Baseline metrics (from earlier)\n",
        "baseline_em = sum(ems) / len(ems)\n",
        "baseline_f1 = sum(f1s) / len(f1s)\n",
        "\n",
        "# Round 1 metrics - re-evaluate for fair comparison\n",
        "base_model_r1_eval = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "model_round1 = PeftModel.from_pretrained(base_model_r1_eval, \"./lora_adapter\")\n",
        "model_round1 = model_round1.to(\"cpu\")\n",
        "model_round1.eval()\n",
        "\n",
        "ems_r1, f1s_r1 = [], []\n",
        "for item in eval_data:\n",
        "    pred = generate_answer_eval(model_round1, tokenizer_eval, item[\"question\"])\n",
        "    ems_r1.append(exact_match(pred, item[\"answer\"]))\n",
        "    f1s_r1.append(f1_score(pred, item[\"answer\"]))\n",
        "\n",
        "round1_em = sum(ems_r1) / len(ems_r1)\n",
        "round1_f1 = sum(f1s_r1) / len(f1s_r1)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    \"Stage\": [\"Baseline (GPT-2)\", \"Round 1 (All Generic)\", \"Round 2 (Curated Targeted)\"],\n",
        "    \"Exact Match\": [baseline_em, round1_em, round2_em],\n",
        "    \"F1 Score\": [baseline_f1, round1_f1, round2_f1],\n",
        "    \"Training Data\": [\"None\", f\"{len(self_edits)} generic edits\", f\"{len(curated_edits)} targeted edits\"],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(comparison_data)\n",
        "results_df[\"ΔF1 vs Baseline\"] = results_df[\"F1 Score\"] - baseline_f1\n",
        "results_df[\"ΔEM vs Baseline\"] = results_df[\"Exact Match\"] - baseline_em\n",
        "\n",
        "print(\"=\" * 75)\n",
        "print(\"📊 SEAL EXPERIMENT RESULTS - FINAL COMPARISON\")\n",
        "print(\"=\" * 75)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Summary insights\n",
        "print(\"\\n📈 Key Insights:\")\n",
        "print(f\"   • Baseline F1:                    {baseline_f1:.4f}\")\n",
        "print(f\"   • Round 1 F1 (generic edits):     {round1_f1:.4f} ({(round1_f1 - baseline_f1):+.4f})\")\n",
        "print(f\"   • Round 2 F1 (targeted edits):    {round2_f1:.4f} ({(round2_f1 - baseline_f1):+.4f})\")\n",
        "print(f\"   • Round 2 vs Round 1:             {(round2_f1 - round1_f1):+.4f}\")\n",
        "\n",
        "if round2_f1 > round1_f1:\n",
        "    improvement_pct = ((round2_f1 - round1_f1) / round1_f1) * 100 if round1_f1 > 0 else 0\n",
        "    print(f\"\\n✅ SUCCESS! Targeted edits improved F1 by {improvement_pct:.1f}% over generic edits!\")\n",
        "elif round2_f1 == round1_f1:\n",
        "    print(\"\\n➡️ No significant difference - both approaches perform similarly\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Generic edits performed better - model may need more diverse training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab4338d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated 10 new self-edits for Round 3\n",
            "   Saved to self_edits_round2.json\n",
            "\n",
            "📝 Preview of new self-edits:\n",
            "\n",
            "1. Question: What training method do LLMs use? Answer: LLMs are trained u...\n",
            "\n",
            "2. Question: When did LLMs first appear? Answer: Large language models ap...\n",
            "\n",
            "3. Question: What is the typical parameter count for an LLM? Answer: LLMs...\n",
            "   ...\n"
          ]
        }
      ],
      "source": [
        "# Simple visualization of results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "stages = [\"Baseline\\n(GPT-2)\", \"Round 1\\n(Generic)\", \"Round 2\\n(Targeted)\"]\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "# F1 Score comparison\n",
        "bars1 = axes[0].bar(stages, [baseline_f1, round1_f1, round2_f1], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0].set_ylabel('F1 Score', fontsize=12)\n",
        "axes[0].set_title('F1 Score Across SEAL Rounds', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, max(baseline_f1, round1_f1, round2_f1) * 1.4)\n",
        "for i, v in enumerate([baseline_f1, round1_f1, round2_f1]):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "# Add improvement annotation\n",
        "if round2_f1 > round1_f1:\n",
        "    axes[0].annotate('', xy=(2, round2_f1), xytext=(1, round1_f1),\n",
        "                     arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
        "    axes[0].text(1.5, (round1_f1 + round2_f1)/2, f'+{(round2_f1-round1_f1):.4f}', \n",
        "                 color='green', fontweight='bold', ha='center')\n",
        "\n",
        "# Exact Match comparison  \n",
        "bars2 = axes[1].bar(stages, [baseline_em, round1_em, round2_em], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1].set_ylabel('Exact Match', fontsize=12)\n",
        "axes[1].set_title('Exact Match Across SEAL Rounds', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim(0, max(0.15, max(baseline_em, round1_em, round2_em) * 1.4))\n",
        "for i, v in enumerate([baseline_em, round1_em, round2_em]):\n",
        "    axes[1].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "# Add legend explaining the approach\n",
        "fig.text(0.5, 0.02, \n",
        "         'Round 1: All 20 generic self-edits | Round 2: 16 targeted Q&A pairs matching eval format',\n",
        "         ha='center', fontsize=10, style='italic')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
        "plt.savefig('seal_results_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Chart saved to seal_results_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1272fa6",
      "metadata": {},
      "source": [
        "## Summary & Reflections\n",
        "\n",
        "### What I implemented (Updated):\n",
        "1. **Baseline evaluation** - Measured GPT-2's performance before any SEAL training\n",
        "2. **Round 1** - Fine-tuned on ALL generic self-edits (20 edits)\n",
        "3. **Edit Analysis** - Found all edits had identical ΔF1 (0.021) - filtering was useless!\n",
        "4. **Problem Diagnosis** - Analyzed evaluation questions to understand what topics matter\n",
        "5. **Round 2 (Improved)** - Created TARGETED edits matching eval Q&A format exactly\n",
        "6. **Comparison** - Tracked metrics to see if targeted approach helped\n",
        "\n",
        "### Key Lesson Learned:\n",
        "The original Round 1 edits were too **generic** - they paraphrased the passage but didn't match the specific Q&A format of evaluation. By creating edits that:\n",
        "- Use the exact \"Question: ... Answer: ...\" format\n",
        "- Cover the same topics as evaluation questions\n",
        "- Have concise, factual answers\n",
        "\n",
        "...I gave the model a much better signal for what pattern to learn.\n",
        "\n",
        "### Connection to SEAL Paper:\n",
        "- This matches the paper's insight that **self-edit quality matters more than quantity**\n",
        "- The Student-Teacher approach allows creating high-quality targeted edits\n",
        "- The key is aligning training data (SE) with the downstream task (τ)\n",
        "\n",
        "### What I would do differently next time:\n",
        "- Start by analyzing evaluation questions BEFORE generating self-edits\n",
        "- Use fewer, higher-quality edits rather than many generic ones\n",
        "- Match the exact format expected in evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0232c19c24da4e9f8df2179c111a7e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f792fb2141f4a22b35d20c181900003",
              "IPY_MODEL_dbe83febc4de4fd6917761deae175930",
              "IPY_MODEL_195d8255427346a683026582b757a100"
            ],
            "layout": "IPY_MODEL_eb07a860627644a9be64e57970c840ef"
          }
        },
        "023924ba6c4747c6bc4695a34fbb71b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63ea82e7a0041aea334637db6391aee",
            "placeholder": "​",
            "style": "IPY_MODEL_5b000843c60d44f9a9814ff0161d83a0",
            "value": " 20/20 [00:00&lt;00:00, 166.27 examples/s]"
          }
        },
        "072330432dac4e02bde1e3b169e3a678": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "080883d33fe440bea4a1c2e9f9edec91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a53df2b1c34d37b6f608e39494210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ad973c5ce764830b5b0b3ff52ec0000": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7edd449451467ba303a1dd5c7e5783": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108fdf8ae8d64cadbe6326ad207b628a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42f0a029560c485688a7f9eb5ce452a2",
              "IPY_MODEL_b920fe0643394fa099662c30acdfe12c",
              "IPY_MODEL_aafb73498d5b4b39ac8098789d768444"
            ],
            "layout": "IPY_MODEL_c930e3190bd54c4397db3e93aa85e360"
          }
        },
        "14eeb44ec7a74155a694ae07e1a45864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16133075009c4b0ea57f5ea66da5f254": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53706dee1c6f4713bfd87bbfd8080db5",
            "placeholder": "​",
            "style": "IPY_MODEL_bd80fe036005413c925f0aaac4690953",
            "value": " 665/665 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "195d8255427346a683026582b757a100": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f14946dd400a47bf96264742465cc745",
            "placeholder": "​",
            "style": "IPY_MODEL_971fc050bce24581bbd9c09022221133",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.22kB/s]"
          }
        },
        "21b748514ac14b5e824eb717734a1000": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8eebf672ce43f3989ba85e9cff4d08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3185bd9d1a3a4014b5e87efc378ce1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "367d648498b34cbdba348ae91f889fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5dbb38f90f34b9f8622f8a26effb075",
            "placeholder": "​",
            "style": "IPY_MODEL_f2796b02ffb04ffebe095108b30bc94c",
            "value": " 456k/? [00:00&lt;00:00, 20.6MB/s]"
          }
        },
        "381f490b32354187935305f4bbadf181": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a69018175c342ffa14bb6eb9aa13822": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf7b40eb4ff46b1a5ab4f94cc769ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41022d5032b449358767037ce9f9bd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5aeafafa7564af9ac636ce8b00dabca",
              "IPY_MODEL_eeb004929d0443d8bfd61f3d9b41989f",
              "IPY_MODEL_367d648498b34cbdba348ae91f889fa1"
            ],
            "layout": "IPY_MODEL_0ad973c5ce764830b5b0b3ff52ec0000"
          }
        },
        "42f0a029560c485688a7f9eb5ce452a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_072330432dac4e02bde1e3b169e3a678",
            "placeholder": "​",
            "style": "IPY_MODEL_9b389192f72442edae89d4fc4dda0d2d",
            "value": "vocab.json: "
          }
        },
        "4a312d37c78d4e21abaa71716b4d117f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc236f0acc246108bea19d9f2f2bef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5067c5076ad14f8c82c74765ba9b94ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e6d26176fb41a88fe4be0cb07b2f76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a558d812354b308d660d66f38072a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53706dee1c6f4713bfd87bbfd8080db5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58cdcca04faf4192a6fe6d1b61ecfac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b000843c60d44f9a9814ff0161d83a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de1968be2994401bb3f246adc0424b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f792fb2141f4a22b35d20c181900003": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dde50abc60434cc485772b233b999e81",
            "placeholder": "​",
            "style": "IPY_MODEL_9d47dc477c3944b2b7d78efd69286425",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "5f94a922626144818c81b8f878030199": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64105a02574e4dc09141f7faf775a281": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69288e8f3df240fe8639fd52be062943": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a38b0c728c04a9aa84963a0e0132b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7edd449451467ba303a1dd5c7e5783",
            "placeholder": "​",
            "style": "IPY_MODEL_75a1079ecd0a4e10922b044b2e544389",
            "value": "model.safetensors: 100%"
          }
        },
        "6fbfdf150ee54f809549333cfdefa928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74a92812f2894b038a69add20f0d1548": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b748514ac14b5e824eb717734a1000",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6f294d4678542e1b4d8673884f6005c",
            "value": 548105171
          }
        },
        "75a1079ecd0a4e10922b044b2e544389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75a29fb033bb481487fd5415421434bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a312d37c78d4e21abaa71716b4d117f",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_381f490b32354187935305f4bbadf181",
            "value": 665
          }
        },
        "78815a54235d49a4bcad04fb6ba2b887": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c834f50830034447a7c5c6b01d44e682",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64105a02574e4dc09141f7faf775a281",
            "value": 20
          }
        },
        "7a1b58675c184ebdb1c40c66633967fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_080883d33fe440bea4a1c2e9f9edec91",
            "placeholder": "​",
            "style": "IPY_MODEL_9f764ad6026442b88efa2fcc46b74274",
            "value": "config.json: 100%"
          }
        },
        "7c71916ce27643e8b14d61d9de85546b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a1b58675c184ebdb1c40c66633967fd",
              "IPY_MODEL_75a29fb033bb481487fd5415421434bb",
              "IPY_MODEL_16133075009c4b0ea57f5ea66da5f254"
            ],
            "layout": "IPY_MODEL_5f94a922626144818c81b8f878030199"
          }
        },
        "7f0236103fd84cc98f6ed9e5960b3ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7facac77cf984cddbe7a67f7c700c1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "843efad48f124c9184bd2cc22e468747": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a38b0c728c04a9aa84963a0e0132b2e",
              "IPY_MODEL_74a92812f2894b038a69add20f0d1548",
              "IPY_MODEL_b30905721ca043d58a23b8829e5c06d9"
            ],
            "layout": "IPY_MODEL_6fbfdf150ee54f809549333cfdefa928"
          }
        },
        "895a21d5b73149aa86986c7b0afa6529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a429b21b18244cda87c8f9419a1b7a1",
              "IPY_MODEL_78815a54235d49a4bcad04fb6ba2b887",
              "IPY_MODEL_023924ba6c4747c6bc4695a34fbb71b9"
            ],
            "layout": "IPY_MODEL_dc77766adcdf477786fee15537468ae4"
          }
        },
        "89def4fbc98d46cb9d1f8bcc6ad5a39f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b335835ede44553bb23b1aa77009371": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14eeb44ec7a74155a694ae07e1a45864",
            "placeholder": "​",
            "style": "IPY_MODEL_58cdcca04faf4192a6fe6d1b61ecfac9",
            "value": "generation_config.json: 100%"
          }
        },
        "91ac5ce6d74649168cbafc5c2fcb4633": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d75767c1054a0da0b33f3b10021f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb969ee42b95406a85a300633725fcd9",
              "IPY_MODEL_d5aa100d4bca4593a50e3ef05494e404",
              "IPY_MODEL_badd10a2479241699638e2718537f2f5"
            ],
            "layout": "IPY_MODEL_89def4fbc98d46cb9d1f8bcc6ad5a39f"
          }
        },
        "971fc050bce24581bbd9c09022221133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a429b21b18244cda87c8f9419a1b7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a558d812354b308d660d66f38072a2",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf7b40eb4ff46b1a5ab4f94cc769ccf",
            "value": "Map: 100%"
          }
        },
        "9b389192f72442edae89d4fc4dda0d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d47dc477c3944b2b7d78efd69286425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e71e0ea3f1c4a5cbe09ffa1fb78b400": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f764ad6026442b88efa2fcc46b74274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2538136e85c4d408453b246383135f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e6d26176fb41a88fe4be0cb07b2f76",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69288e8f3df240fe8639fd52be062943",
            "value": 124
          }
        },
        "aafb73498d5b4b39ac8098789d768444": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f8eebf672ce43f3989ba85e9cff4d08",
            "placeholder": "​",
            "style": "IPY_MODEL_e3d9a79e97724a79a11661c19573a97e",
            "value": " 1.04M/? [00:00&lt;00:00, 26.9MB/s]"
          }
        },
        "b30905721ca043d58a23b8829e5c06d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e142aea531ed413fa6d09afa407e75a6",
            "placeholder": "​",
            "style": "IPY_MODEL_09a53df2b1c34d37b6f608e39494210d",
            "value": " 548M/548M [00:06&lt;00:00, 76.6MB/s]"
          }
        },
        "b63ea82e7a0041aea334637db6391aee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b920fe0643394fa099662c30acdfe12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a6c27d34c3431e976fb374a1393f24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fc236f0acc246108bea19d9f2f2bef2",
            "value": 1
          }
        },
        "badd10a2479241699638e2718537f2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66761f6eb0d4358a86b91fad85388a1",
            "placeholder": "​",
            "style": "IPY_MODEL_7facac77cf984cddbe7a67f7c700c1b5",
            "value": " 1.36M/? [00:00&lt;00:00, 38.7MB/s]"
          }
        },
        "bb969ee42b95406a85a300633725fcd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a69018175c342ffa14bb6eb9aa13822",
            "placeholder": "​",
            "style": "IPY_MODEL_d7c9d6dd5e0442a69b58f48b208842ed",
            "value": "tokenizer.json: "
          }
        },
        "bc68a29154234c5598a2183ce28be1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd80fe036005413c925f0aaac4690953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8dfcd43edb4c2191818db5bff6dfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc68a29154234c5598a2183ce28be1f2",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa71254746f40c28d77f2fc51b5fa6c",
            "value": " 124/124 [00:00&lt;00:00, 5.99kB/s]"
          }
        },
        "c834f50830034447a7c5c6b01d44e682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c930e3190bd54c4397db3e93aa85e360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d381ce6ca85c496292466d2930a14d74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d5aa100d4bca4593a50e3ef05494e404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d381ce6ca85c496292466d2930a14d74",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3185bd9d1a3a4014b5e87efc378ce1a3",
            "value": 1
          }
        },
        "d66761f6eb0d4358a86b91fad85388a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c9d6dd5e0442a69b58f48b208842ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe83febc4de4fd6917761deae175930": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fae0d30363f845a6a703d919069c99de",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5de1968be2994401bb3f246adc0424b1",
            "value": 26
          }
        },
        "dc77766adcdf477786fee15537468ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde50abc60434cc485772b233b999e81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e142aea531ed413fa6d09afa407e75a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d9a79e97724a79a11661c19573a97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5aeafafa7564af9ac636ce8b00dabca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91ac5ce6d74649168cbafc5c2fcb4633",
            "placeholder": "​",
            "style": "IPY_MODEL_9e71e0ea3f1c4a5cbe09ffa1fb78b400",
            "value": "merges.txt: "
          }
        },
        "e5dbb38f90f34b9f8622f8a26effb075": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f294d4678542e1b4d8673884f6005c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaa71254746f40c28d77f2fc51b5fa6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb07a860627644a9be64e57970c840ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb004929d0443d8bfd61f3d9b41989f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0236103fd84cc98f6ed9e5960b3ca1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc90c8ee83234de592832968a05d56d7",
            "value": 1
          }
        },
        "f14946dd400a47bf96264742465cc745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2409dc38be041df8ce3ebafac7512bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b335835ede44553bb23b1aa77009371",
              "IPY_MODEL_a2538136e85c4d408453b246383135f4",
              "IPY_MODEL_bd8dfcd43edb4c2191818db5bff6dfa9"
            ],
            "layout": "IPY_MODEL_5067c5076ad14f8c82c74765ba9b94ec"
          }
        },
        "f2796b02ffb04ffebe095108b30bc94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7a6c27d34c3431e976fb374a1393f24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fae0d30363f845a6a703d919069c99de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc90c8ee83234de592832968a05d56d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
